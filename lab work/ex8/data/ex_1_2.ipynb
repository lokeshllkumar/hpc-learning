{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIkrsN5BWAID",
        "outputId": "6aabd2cb-f91c-474d-de2f-afe1acfab80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  8 05:47:28 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXFXbd_WWEEe",
        "outputId": "d9de3062-67b4-4c9f-97e9-fef21fa151d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz4SMYn4SxmZ",
        "outputId": "e8230b1f-be23-46c6-f1d1-c0c8a6bf38cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cuda-samples' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO0Z6Gd5Um7S",
        "outputId": "eeefeaa0-8a3d-498a-a71f-4885e26f8e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make: Nothing to be done for 'all'.\n"
          ]
        }
      ],
      "source": [
        "!cd cuda-samples/Samples/1_Utilities/deviceQuery && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dqe8QhuUtUT",
        "outputId": "c694eb7b-9ec3-48b6-9040-5859e3a05946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deviceQuery\t deviceQuery_vs2017.sln      deviceQuery_vs2019.vcxproj  Makefile\n",
            "deviceQuery.cpp  deviceQuery_vs2017.vcxproj  deviceQuery_vs2022.sln\t NsightEclipse.xml\n",
            "deviceQuery.o\t deviceQuery_vs2019.sln      deviceQuery_vs2022.vcxproj  README.md\n",
            "cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.2, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ],
      "source": [
        "!cd cuda-samples/Samples/1_Utilities/deviceQuery && ls\n",
        "!cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwsdgr2wUvtY",
        "outputId": "15be8660-1ea2-4543-cf37-db2e43368c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL502K4GUziy",
        "outputId": "7faa75f4-117a-4f98-bcce-09d6eaf1321c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWxD_avdUJ2d",
        "outputId": "83194463-4be0-4500-934e-b5979a7705a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\tp\tTime\n",
            "4\t3\t0.000234\n",
            "8\t3\t0.000016\n",
            "16\t3\t0.000013\n",
            "32\t3\t0.000012\n",
            "64\t3\t0.000012\n",
            "128\t3\t0.000012\n",
            "256\t3\t0.000013\n",
            "512\t3\t0.000012\n",
            "1024\t3\t0.000012\n",
            "2048\t3\t0.000013\n",
            "4096\t3\t0.000012\n",
            "8192\t3\t0.000013\n",
            "16384\t3\t0.000010\n",
            "32768\t3\t0.000017\n",
            "4\t13\t0.000032\n",
            "8\t13\t0.000031\n",
            "16\t13\t0.000031\n",
            "32\t13\t0.000031\n",
            "64\t13\t0.000033\n",
            "128\t13\t0.000034\n",
            "256\t13\t0.000037\n",
            "512\t13\t0.000037\n",
            "1024\t13\t0.000037\n",
            "2048\t13\t0.000036\n",
            "4096\t13\t0.000036\n",
            "8192\t13\t0.000033\n",
            "16384\t13\t0.000035\n",
            "32768\t13\t0.000060\n",
            "4\t23\t0.000055\n",
            "8\t23\t0.000057\n",
            "16\t23\t0.000060\n",
            "32\t23\t0.000062\n",
            "64\t23\t0.000063\n",
            "128\t23\t0.000065\n",
            "256\t23\t0.000068\n",
            "512\t23\t0.000065\n",
            "1024\t23\t0.000063\n",
            "2048\t23\t0.000063\n",
            "4096\t23\t0.000068\n",
            "8192\t23\t0.000067\n",
            "16384\t23\t0.000077\n",
            "32768\t23\t0.000142\n",
            "4\t33\t0.000089\n",
            "8\t33\t0.000094\n",
            "16\t33\t0.000096\n",
            "32\t33\t0.000097\n",
            "64\t33\t0.000100\n",
            "128\t33\t0.000097\n",
            "256\t33\t0.000099\n",
            "512\t33\t0.000097\n",
            "1024\t33\t0.000097\n",
            "2048\t33\t0.000099\n",
            "4096\t33\t0.000095\n",
            "8192\t33\t0.000098\n",
            "16384\t33\t0.000136\n",
            "32768\t33\t0.000246\n",
            "4\t43\t0.000117\n",
            "8\t43\t0.000121\n",
            "16\t43\t0.000125\n",
            "32\t43\t0.000129\n",
            "64\t43\t0.000130\n",
            "128\t43\t0.000126\n",
            "256\t43\t0.000138\n",
            "512\t43\t0.000141\n",
            "1024\t43\t0.000134\n",
            "2048\t43\t0.000140\n",
            "4096\t43\t0.000137\n",
            "8192\t43\t0.000139\n",
            "16384\t43\t0.000207\n",
            "32768\t43\t0.000392\n",
            "4\t53\t0.000152\n",
            "8\t53\t0.000156\n",
            "16\t53\t0.000161\n",
            "32\t53\t0.000164\n",
            "64\t53\t0.000164\n",
            "128\t53\t0.000161\n",
            "256\t53\t0.000185\n",
            "512\t53\t0.000184\n",
            "1024\t53\t0.000188\n",
            "2048\t53\t0.000188\n",
            "4096\t53\t0.000183\n",
            "8192\t53\t0.000187\n",
            "16384\t53\t0.000300\n",
            "32768\t53\t0.000572\n",
            "4\t63\t0.000192\n",
            "8\t63\t0.000196\n",
            "16\t63\t0.000200\n",
            "32\t63\t0.000205\n",
            "64\t63\t0.000205\n",
            "128\t63\t0.000197\n",
            "256\t63\t0.000247\n",
            "512\t63\t0.000244\n",
            "1024\t63\t0.000242\n",
            "2048\t63\t0.000244\n",
            "4096\t63\t0.000234\n",
            "8192\t63\t0.000246\n",
            "16384\t63\t0.000408\n",
            "32768\t63\t0.000779\n",
            "4\t73\t0.000236\n",
            "8\t73\t0.000241\n",
            "16\t73\t0.000244\n",
            "32\t73\t0.000247\n",
            "64\t73\t0.000248\n",
            "128\t73\t0.000242\n",
            "256\t73\t0.000305\n",
            "512\t73\t0.000311\n",
            "1024\t73\t0.000315\n",
            "2048\t73\t0.000301\n",
            "4096\t73\t0.000305\n",
            "8192\t73\t0.000302\n",
            "16384\t73\t0.000527\n",
            "32768\t73\t0.001010\n",
            "4\t83\t0.000284\n",
            "8\t83\t0.000290\n",
            "16\t83\t0.000293\n",
            "32\t83\t0.000296\n",
            "64\t83\t0.000295\n",
            "128\t83\t0.000287\n",
            "256\t83\t0.000379\n",
            "512\t83\t0.000379\n",
            "1024\t83\t0.000387\n",
            "2048\t83\t0.000377\n",
            "4096\t83\t0.000372\n",
            "8192\t83\t0.000370\n",
            "16384\t83\t0.000671\n",
            "32768\t83\t0.001294\n",
            "4\t93\t0.000336\n",
            "8\t93\t0.000342\n",
            "16\t93\t0.000347\n",
            "32\t93\t0.000349\n",
            "64\t93\t0.000347\n",
            "128\t93\t0.000338\n",
            "256\t93\t0.000471\n",
            "512\t93\t0.000465\n",
            "1024\t93\t0.000463\n",
            "2048\t93\t0.000462\n",
            "4096\t93\t0.000466\n",
            "8192\t93\t0.000458\n",
            "16384\t93\t0.000827\n",
            "32768\t93\t0.001600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__device__ long long factCalc_gpu(int n) {\n",
        "    if (n == 0) return 1;\n",
        "    long long fact = 1;\n",
        "    for (int i = 1; i <= n; i++) {\n",
        "        fact *= i;\n",
        "    }\n",
        "    return fact;\n",
        "}\n",
        "\n",
        "__device__ float sinApprox_gpu(float x, int p) {\n",
        "    float res = 0.0;\n",
        "    for (int i = 0; i < p; i++) {\n",
        "        int exp = 2 * i + 1;\n",
        "        float term = powf(-1, i) * powf(x, exp) / factCalc_gpu(exp);\n",
        "        res += term;\n",
        "    }\n",
        "    return res;\n",
        "}\n",
        "\n",
        "__global__ void gpu_findSin(float* arr, float* res, int N, int p) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        res[i] = sinApprox_gpu(arr[i], p);\n",
        "    }\n",
        "}\n",
        "\n",
        "void testTiming(int N, int p) {\n",
        "\n",
        "    float *arr, *result, *d_arr, *d_result;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    arr = (float *)malloc(size);\n",
        "    result = (float *)malloc(size);\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        arr[i] = (float)i / N;\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void **)&d_arr, size);\n",
        "    cudaMalloc((void **)&d_result, size);\n",
        "\n",
        "    cudaMemcpy(d_arr, arr, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    gpu_findSin<<<blocksPerGrid, threadsPerBlock>>>(d_arr, d_result, N, p);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(result, d_result, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float time = 0;\n",
        "    cudaEventElapsedTime(&time, start, stop);\n",
        "    time /= 1000;\n",
        "\n",
        "    printf(\"%d\\t%d\\t%f\\n\", N, p, time);\n",
        "\n",
        "    cudaFree(d_arr);\n",
        "    cudaFree(d_result);\n",
        "    free(arr);\n",
        "    free(result);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int min_p = 3, max_p = 100;\n",
        "    int min_N = 1 << 2;\n",
        "    int max_N = 1 << 15;\n",
        "\n",
        "    printf(\"N\\tp\\tTime\\n\");\n",
        "    for (int p = min_p; p <= max_p; p += 10) {\n",
        "        for (int N = min_N; N <= max_N; N *= 2) {\n",
        "            testTiming(N, p);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}