{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIkrsN5BWAID",
        "outputId": "600b6129-1db5-4e6a-9116-5e2f4baae9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Nov  3 14:57:27 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   41C    P3             17W /   30W |      14MiB /   4096MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      2321      G   /usr/lib/xorg/Xorg                              4MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXFXbd_WWEEe",
        "outputId": "56408ad1-2a35-400a-8bca-d959abc7709f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Thu_Nov_18_09:45:30_PST_2021\n",
            "Cuda compilation tools, release 11.5, V11.5.119\n",
            "Build cuda_11.5.r11.5/compiler.30672275_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz4SMYn4SxmZ",
        "outputId": "39624559-ea75-4055-e9ae-19bc35df8a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 19507, done.\u001b[K\n",
            "remote: Counting objects: 100% (10080/10080), done.\u001b[K\n",
            "remote: Compressing objects: 100% (569/569), done.\u001b[K\n",
            "remote: Total 19507 (delta 9605), reused 9714 (delta 9511), pack-reused 9427 (from 1)\u001b[K\n",
            "Receiving objects: 100% (19507/19507), 133.56 MiB | 5.80 MiB/s, done.\n",
            "Resolving deltas: 100% (17060/17060), done.\n",
            "Updating files: 100% (4026/4026), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO0Z6Gd5Um7S",
        "outputId": "1d5781aa-1cc5-4067-b9d6-7d3c652a8b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery.o -c deviceQuery.cpp\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../../bin/x86_64/linux/release\n"
          ]
        }
      ],
      "source": [
        "!cd cuda-samples/Samples/1_Utilities/deviceQuery && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dqe8QhuUtUT",
        "outputId": "ad25e394-b979-42cb-bcaa-d73c6148542d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deviceQuery\t\tdeviceQuery_vs2017.vcxproj  deviceQuery_vs2022.vcxproj\n",
            "deviceQuery.cpp\t\tdeviceQuery_vs2019.sln\t    Makefile\n",
            "deviceQuery.o\t\tdeviceQuery_vs2019.vcxproj  NsightEclipse.xml\n",
            "deviceQuery_vs2017.sln\tdeviceQuery_vs2022.sln\t    README.md\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"NVIDIA GeForce RTX 3050 Laptop GPU\"\n",
            "  CUDA Driver Version / Runtime Version          12.6 / 12.6\n",
            "  CUDA Capability Major/Minor version number:    8.6\n",
            "  Total amount of global memory:                 3801 MBytes (3985833984 bytes)\n",
            "  (016) Multiprocessors, (128) CUDA Cores/MP:    2048 CUDA Cores\n",
            "  GPU Max Clock rate:                            1500 MHz (1.50 GHz)\n",
            "  Memory Clock rate:                             6001 Mhz\n",
            "  Memory Bus Width:                              128-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        102400 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1536\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     Yes\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Disabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.6, CUDA Runtime Version = 12.6, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ],
      "source": [
        "!cd cuda-samples/Samples/1_Utilities/deviceQuery && ls\n",
        "!cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwsdgr2wUvtY",
        "outputId": "5bd2dd58-b6f3-426f-80b9-b1fb52948d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nvcc4jupyter in /home/loki/.local/lib/python3.10/site-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL502K4GUziy",
        "outputId": "a021e9e4-6b5e-4864-d054-e80e3a0a5007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source files will be saved in \"/tmp/tmp73rf_o0i\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWxD_avdUJ2d",
        "outputId": "1e94f2ee-2bec-40a1-9e0a-ecfd7d793247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "// 1D convolution of a matrix\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void conv1D(float* input, float* kernel, float* output, int dataSize, int kernelSize) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int halfKernel = kernelSize / 2;\n",
        "    float sum = 0;\n",
        "\n",
        "    if (idx < dataSize) {\n",
        "        for (int j = -halfKernel; j <= halfKernel; j++) {\n",
        "            int index = idx + j;\n",
        "            if (index >= 0 && index < dataSize) {\n",
        "                sum += input[index] * kernel[halfKernel + j];\n",
        "            }\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dataSize = 1024;\n",
        "    int kernelSize = 5;\n",
        "    int size = dataSize * sizeof(float);\n",
        "\n",
        "    float *h_input = new float[dataSize];\n",
        "    float *h_kernel = new float[kernelSize];\n",
        "    float *h_output = new float[dataSize];\n",
        "\n",
        "    float *d_input, *d_kernel, *d_output;\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_kernel, kernelSize * sizeof(float));\n",
        "    cudaMalloc(&d_output, size);\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_kernel, h_kernel, kernelSize * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (dataSize + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    conv1D<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_kernel, d_output, dataSize, kernelSize);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    delete[] h_input;\n",
        "    delete[] h_kernel;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_kernel);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CZXKrW1t3X8s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "// matrix to vector and perform element-wise operation\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixToVector(float* matrix, float* vector, int rows, int cols) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < rows * cols) {\n",
        "        vector[idx] = matrix[idx] * 2.0f; // can use any other operation\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int rows = 32, cols = 32;\n",
        "    int size = rows * cols * sizeof(float);\n",
        "\n",
        "    float *h_matrix = new float[rows * cols];\n",
        "    float *h_vector = new float[rows * cols];\n",
        "\n",
        "    float *d_matrix, *d_vector;\n",
        "    cudaMalloc(&d_matrix, size);\n",
        "    cudaMalloc(&d_vector, size);\n",
        "\n",
        "    cudaMemcpy(d_matrix, h_matrix, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (rows * cols + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    matrixToVector<<<blocksPerGrid, threadsPerBlock>>>(d_matrix, d_vector, rows, cols);\n",
        "\n",
        "    cudaMemcpy(h_vector, d_vector, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    delete[] h_matrix;\n",
        "    delete[] h_vector;\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_vector);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "// apply non-linear function on input(Sigmoid approximation)\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void applySigmoid(float* input, float* output, int size) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = 1.0f / (1.0f + expf(-input[idx]));\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 1024;\n",
        "    float *h_input = new float[size];\n",
        "    float *h_output = new float[size];\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, size * sizeof(float));\n",
        "    cudaMalloc(&d_output, size * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    applySigmoid<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, size);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "// stream compaction(can convert based on boolean conditions)\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void streamCompactStage1(float* input, int* output, int size) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = (input[idx] > 0.5f) ? 1 : 0;  // Example boolean condition\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 1024;\n",
        "    float *h_input = new float[size];\n",
        "    int *h_output = new int[size];\n",
        "\n",
        "    float *d_input;\n",
        "    int *d_output;\n",
        "    cudaMalloc(&d_input, size * sizeof(float));\n",
        "    cudaMalloc(&d_output, size * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    streamCompactStage1<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, size);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "\n",
        "// matrix-vector multiplication\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixVectorMultiply(float* matrix, float* vector, float* output, int rows, int cols) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (row < rows) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            sum += matrix[row * cols + j] * vector[j];\n",
        "        }\n",
        "        output[row] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int rows = 32, cols = 32;\n",
        "    float *h_matrix = new float[rows * cols];\n",
        "    float *h_vector = new float[cols];\n",
        "    float *h_output = new float[rows];\n",
        "\n",
        "    float *d_matrix, *d_vector, *d_output;\n",
        "    cudaMalloc(&d_matrix, rows * cols * sizeof(float));\n",
        "    cudaMalloc(&d_vector, cols * sizeof(float));\n",
        "    cudaMalloc(&d_output, rows * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_matrix, h_matrix, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector, h_vector, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (rows + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    matrixVectorMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_matrix, d_vector, d_output, rows, cols);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, rows * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    delete[] h_matrix;\n",
        "    delete[] h_vector;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "\n",
        "// reduction on a matrix(matrix to vector)\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixReduction(float* input, float* output, int size) {\n",
        "    extern __shared__ float sharedData[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    sharedData[tid] = (idx < size) ? input[idx] : 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int rows = 32, cols = 32;\n",
        "    int size = rows * cols;\n",
        "    float *h_matrix = new float[size];\n",
        "    float *h_output = new float[(size + 255) / 256];\n",
        "\n",
        "    float *d_matrix, *d_partialOutput;\n",
        "    cudaMalloc(&d_matrix, size * sizeof(float));\n",
        "    cudaMalloc(&d_partialOutput, ((size + 255) / 256) * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_matrix, h_matrix, size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    matrixReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(float)>>>(d_matrix, d_partialOutput, size);\n",
        "\n",
        "    cudaMemcpy(h_output, d_partialOutput, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // perform only if needed or specified in the question\n",
        "    float totalSum = 0.0f;\n",
        "    for (int i = 0; i < blocksPerGrid; i++) {\n",
        "        totalSum += h_output[i];\n",
        "    }\n",
        "\n",
        "    delete[] h_matrix;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_partialOutput);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "\n",
        "// matrix transpose using shared memory\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define TILE_DIM 32\n",
        "#define BLOCK_ROWS 8\n",
        "\n",
        "__global__ void transposeSharedMemory(float *input, float *output, int width, int height) {\n",
        "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
        "\n",
        "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = input[y * width + x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
        "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "    if (x < height && y < width) {\n",
        "        output[y * height + x] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = 64;\n",
        "    int height = 64;\n",
        "\n",
        "    float *h_input = new float[width * height];\n",
        "    float *h_output = new float[width * height];\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_input[i] = static_cast<float>(i);\n",
        "    }\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, width * height * sizeof(float));\n",
        "    cudaMalloc(&d_output, width * height * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threadsPerBlock(TILE_DIM, BLOCK_ROWS);\n",
        "    dim3 blocksPerGrid((width + TILE_DIM - 1) / TILE_DIM, (height + TILE_DIM - 1) / TILE_DIM);\n",
        "\n",
        "    transposeSharedMemory<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, width, height);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Transposed matrix:\\n\";\n",
        "    for (int row = 0; row < 8; row++) {\n",
        "        for (int col = 0; col < 8; col++) {\n",
        "            std::cout << h_output[row * height + col] << \" \";\n",
        "        }\n",
        "        std::cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "\n",
        "// matrix-matrix multiplication using standard CUDA kernel with flattened matrices\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixMultiply(float* A, float* B, float* C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < n; k++) {\n",
        "            sum += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 1024;\n",
        "\n",
        "    float h_A[size], h_B[size], h_C[size];\n",
        "\n",
        "    /*\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_A[i] = static_cast<float>(i % 100);\n",
        "        h_B[i] = static_cast<float>((i + 1) % 100);\n",
        "    }\n",
        "    */\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size * sizeof(float));\n",
        "    cudaMalloc(&d_B, size * sizeof(float));\n",
        "    cudaMalloc(&d_C, size * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((size + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (size + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "\n",
        "// using the Thrust library to perform matrix-vector multiplication\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/transform.h>\n",
        "#include <thrust/functional.h>\n",
        "#include <thrust/inner_product.h>\n",
        "\n",
        "const int rows = 32;\n",
        "const int cols = 32;\n",
        "\n",
        "struct RowDotProduct {\n",
        "    float* vector;\n",
        "    int cols;\n",
        "\n",
        "    RowDotProduct(float* vec, int cols) : vector(vec), cols(cols) {}\n",
        "\n",
        "    __device__ float operator()(const int& row_idx) const {\n",
        "        return thrust::inner_product(\n",
        "            thrust::device_pointer_cast(vector),\n",
        "            thrust::device_pointer_cast(vector + cols),\n",
        "            thrust::device_pointer_cast(&vector[row_idx * cols]),\n",
        "            0.0f\n",
        "        );\n",
        "    }\n",
        "};\n",
        "\n",
        "int main() {\n",
        "    float h_matrix[rows * cols];\n",
        "    float h_vector[cols];\n",
        "    float h_output[rows];\n",
        "\n",
        "    thrust::device_vector<float> d_matrix(h_matrix, h_matrix + rows * cols);\n",
        "    thrust::device_vector<float> d_vector(h_vector, h_vector + cols);\n",
        "    thrust::device_vector<float> d_output(rows);\n",
        "\n",
        "    thrust::transform(\n",
        "        thrust::make_counting_iterator(0),\n",
        "        thrust::make_counting_iterator(rows),\n",
        "        d_output.begin(),\n",
        "        RowDotProduct(thrust::raw_pointer_cast(d_vector.data()), cols)\n",
        "    );\n",
        "\n",
        "    thrust::copy(d_output.begin(), d_output.end(), h_output);\n",
        "\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        std::cout << \"Row \" << i << \" result: \" << h_output[i] << std::endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
